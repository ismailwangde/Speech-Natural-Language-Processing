{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-09-19T17:10:34.355465Z","iopub.status.busy":"2024-09-19T17:10:34.355150Z","iopub.status.idle":"2024-09-19T17:10:34.714183Z","shell.execute_reply":"2024-09-19T17:10:34.713242Z","shell.execute_reply.started":"2024-09-19T17:10:34.355424Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/pii-detection-removal-from-educational-data/sample_submission.csv\n","/kaggle/input/pii-detection-removal-from-educational-data/train.json\n","/kaggle/input/pii-detection-removal-from-educational-data/test.json\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-09-19T17:10:34.716355Z","iopub.status.busy":"2024-09-19T17:10:34.715941Z","iopub.status.idle":"2024-09-19T17:11:58.292472Z","shell.execute_reply":"2024-09-19T17:11:58.291221Z","shell.execute_reply.started":"2024-09-19T17:10:34.716320Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.44.0)\n","Collecting transformers\n","  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.24.6)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.4)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\n","Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: transformers\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.44.0\n","    Uninstalling transformers-4.44.0:\n","      Successfully uninstalled transformers-4.44.0\n","Successfully installed transformers-4.44.2\n","Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.33.0)\n","Collecting accelerate\n","  Downloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\n","Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.2)\n","Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.4.0)\n","Requirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.24.6)\n","Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.4)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.15.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.4)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.2)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.2)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.7.4)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Downloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: accelerate\n","  Attempting uninstall: accelerate\n","    Found existing installation: accelerate 0.33.0\n","    Uninstalling accelerate-0.33.0:\n","      Successfully uninstalled accelerate-0.33.0\n","Successfully installed accelerate-0.34.2\n","Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.21.0)\n","Collecting datasets\n","  Downloading datasets-3.0.0-py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\n","Requirement already satisfied: huggingface-hub>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.24.6)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Downloading datasets-3.0.0-py3-none-any.whl (474 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m474.3/474.3 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: datasets\n","  Attempting uninstall: datasets\n","    Found existing installation: datasets 2.21.0\n","    Uninstalling datasets-2.21.0:\n","      Successfully uninstalled datasets-2.21.0\n","Successfully installed datasets-3.0.0\n"]}],"source":["!pip install -U transformers\n","!pip install -U datasets\n","!pip install seqeval evaluate -q"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-09-19T17:11:58.294313Z","iopub.status.busy":"2024-09-19T17:11:58.293961Z","iopub.status.idle":"2024-09-19T17:12:01.285246Z","shell.execute_reply":"2024-09-19T17:12:01.284411Z","shell.execute_reply.started":"2024-09-19T17:11:58.294276Z"},"trusted":true},"outputs":[],"source":["import json\n","train=json.load(open('/kaggle/input/pii-detection-removal-from-educational-data/train.json'))\n","test=json.load(open('/kaggle/input/pii-detection-removal-from-educational-data/test.json'))"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-09-19T17:12:01.287716Z","iopub.status.busy":"2024-09-19T17:12:01.287399Z","iopub.status.idle":"2024-09-19T17:12:01.317483Z","shell.execute_reply":"2024-09-19T17:12:01.316777Z","shell.execute_reply.started":"2024-09-19T17:12:01.287683Z"},"trusted":true},"outputs":[],"source":["def process_data(data):\n","  all_texts=[]\n","  all_labels=[]\n","\n","  for doc in data:\n","    tokens=doc['tokens']\n","    labels = doc.get('labels',['O']*len(tokens)) #Use 'O' for missing labels\n","    all_texts.append(tokens)\n","    all_labels.append(labels)\n","\n","  return all_texts,all_labels\n","\n","train_texts, train_labels = process_data(train)\n","test_texts, test_labels = process_data(test)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-09-19T17:12:01.318872Z","iopub.status.busy":"2024-09-19T17:12:01.318584Z","iopub.status.idle":"2024-09-19T17:12:03.951399Z","shell.execute_reply":"2024-09-19T17:12:03.950637Z","shell.execute_reply.started":"2024-09-19T17:12:01.318841Z"},"trusted":true},"outputs":[],"source":["from datasets import Dataset\n","\n","# Convert to huggingface dataset format\n","train_dataset = Dataset.from_dict({'tokens': train_texts, 'ner_tags': train_labels})\n","test_dataset = Dataset.from_dict({'tokens': test_texts})"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-09-19T17:12:03.952832Z","iopub.status.busy":"2024-09-19T17:12:03.952514Z","iopub.status.idle":"2024-09-19T17:12:03.958452Z","shell.execute_reply":"2024-09-19T17:12:03.957574Z","shell.execute_reply.started":"2024-09-19T17:12:03.952799Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["dict_keys(['document', 'full_text', 'tokens', 'trailing_whitespace', 'labels'])\n","full_text: Desig\n","tokens: ['Design', 'Thinking', 'for', 'innovation', 'reflexion']\n","trailing_whitespace: [True, True, True, True, False]\n","labels: ['O', 'O', 'O', 'O', 'O']\n"]}],"source":["print(train[0].keys())\n","for key in train[0].keys():\n","    value = train[0][key]\n","    # Check if the value is a list or a string\n","    if isinstance(value, (list, str)):\n","        print(f\"{key}: {value[:5]}\")"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-09-19T17:12:03.960227Z","iopub.status.busy":"2024-09-19T17:12:03.959905Z","iopub.status.idle":"2024-09-19T17:12:03.974053Z","shell.execute_reply":"2024-09-19T17:12:03.973210Z","shell.execute_reply.started":"2024-09-19T17:12:03.960194Z"},"trusted":true},"outputs":[{"data":{"text/plain":["\"Design Thinking for innovation reflexion-Avril 2021-Nathalie Sylla\\n\\nChallenge & selection\\n\\nThe tool I use to help all stakeholders finding their way through the complexity of a project is the  mind map.\\n\\nWhat exactly is a mind map? According to the definition of Buzan T. and Buzan B. (1999, Dessine-moi  l'intelligence. Paris: Les √âditions d'Organisation.), the mind map (or heuristic diagram) is a graphic  representation technique that follows the natural functioning of the mind and allows the brain's  potential to be released. Cf Annex1\\n\\nThis tool has many advantages:\\n\\n‚Ä¢  It is accessible to all and does not require significant material investment and can be done  quickly\\n\\n‚Ä¢  It is scalable\\n\\n‚Ä¢  It allows categorization and linking of information\\n\\n‚Ä¢  It can be applied to any type of situation: notetaking, problem solving, analysis, creation of  new ideas\\n\\n‚Ä¢  It is suitable for all people and is easy to learn\\n\\n‚Ä¢  It is fun and encourages exchanges\\n\\n‚Ä¢  It makes visible the dimension of projects, opportunities, interconnections\\n\\n‚Ä¢  It synthesizes\\n\\n‚Ä¢  It makes the project understandable\\n\\n‚Ä¢  It allows you to explore ideas\\n\\nThe creation of a mind map starts with an idea/problem located at its center. This starting point  generates ideas/work areas, incremented around this center in a radial structure, which in turn is  completed with as many branches as new ideas.\\n\\nThis tool enables creativity and logic to be mobilized, it is a map of the thoughts.\\n\\nCreativity is enhanced because participants feel comfortable with the method.\\n\\nApplication & Insight\\n\\nI start the process of the mind map creation with the stakeholders standing around a large board  (white or paper board). In the center of the board, I write and highlight the topic to design.\\n\\nThrough a series of questions, I guide the stakeholders in modelling the mind map. I adapt the series  of questions according to the topic to be addressed. In the type of questions, we can use: who, what,  when, where, why, how, how much.\\n\\nThe use of the ‚Äúwhy‚Äù is very interesting to understand the origin. By this way, the interviewed person  frees itself from paradigms and thus dares to propose new ideas / ways of functioning. I plan two  hours for a workshop.\\n\\nDesign Thinking for innovation reflexion-Avril 2021-Nathalie Sylla\\n\\nAfter modelling the mind map on paper, I propose to the participants a digital visualization of their  work with the addition of color codes, images and interconnections. This second workshop also lasts  two hours and allows the mind map to evolve. Once familiarized with it, the stakeholders discover  the power of the tool. Then, the second workshop brings out even more ideas and constructive  exchanges between the stakeholders. Around this new mind map, they have learned to work  together and want to make visible the untold ideas.\\n\\nI now present all the projects I manage in this type of format in order to ease rapid understanding for  decision-makers. These presentations are the core of my business models. The decision-makers are  thus able to identify the opportunities of the projects and can take quick decisions to validate them.  They find answers to their questions thank to a schematic representation.\\n\\nApproach\\n\\nWhat I find amazing with the facilitation of this type of workshop is the participants commitment for  the project. This tool helps to give meaning. The participants appropriate the story and want to keep  writing it. Then, they easily become actors or sponsors of the project. A trust relationship is built,  thus facilitating the implementation of related actions.\\n\\nDesign Thinking for innovation reflexion-Avril 2021-Nathalie Sylla\\n\\nAnnex 1: Mind Map Shared facilities project\\n\\n\""]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["train[0]['full_text']"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-09-19T17:12:03.975408Z","iopub.status.busy":"2024-09-19T17:12:03.975132Z","iopub.status.idle":"2024-09-19T17:12:04.056257Z","shell.execute_reply":"2024-09-19T17:12:04.055516Z","shell.execute_reply.started":"2024-09-19T17:12:03.975377Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{0: 'B-EMAIL',\n"," 1: 'B-ID_NUM',\n"," 2: 'B-NAME_STUDENT',\n"," 3: 'B-PHONE_NUM',\n"," 4: 'B-STREET_ADDRESS',\n"," 5: 'B-URL_PERSONAL',\n"," 6: 'B-USERNAME',\n"," 7: 'I-ID_NUM',\n"," 8: 'I-NAME_STUDENT',\n"," 9: 'I-PHONE_NUM',\n"," 10: 'I-STREET_ADDRESS',\n"," 11: 'I-URL_PERSONAL',\n"," 12: 'O'}"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["from itertools import chain\n","\n","all_labels = sorted(list(set(chain(*[x[\"labels\"] for x in train]))))\n","label2id = {l: i for i,l in enumerate(all_labels)}\n","id2label = {v:k for k,v in label2id.items()}\n","\n","target = [item for item in all_labels if item != 'O']\n","\n","id2label"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-09-19T17:12:04.057870Z","iopub.status.busy":"2024-09-19T17:12:04.057537Z","iopub.status.idle":"2024-09-19T17:12:04.064367Z","shell.execute_reply":"2024-09-19T17:12:04.063535Z","shell.execute_reply.started":"2024-09-19T17:12:04.057836Z"},"trusted":true},"outputs":[],"source":["def align_labels_with_tokens(labels, word_ids):\n","    aligned_labels = []\n","    previous_word_id = None\n","    for word_id in word_ids:\n","        if word_id is None:\n","            aligned_labels.append(-100)\n","        elif word_id != previous_word_id:\n","            aligned_labels.append(labels[word_id])\n","        else:\n","            aligned_labels.append(-100)\n","        previous_word_id = word_id\n","    return aligned_labels"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-09-19T17:12:04.067782Z","iopub.status.busy":"2024-09-19T17:12:04.067462Z","iopub.status.idle":"2024-09-19T17:12:51.575309Z","shell.execute_reply":"2024-09-19T17:12:51.574444Z","shell.execute_reply.started":"2024-09-19T17:12:04.067750Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8ab164246d784017878f872abf4a2f18","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f61f7796e1724076bddc88432a7bae62","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/580 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5e07e0b289b34cdf9e43ae43c59ec580","version_major":2,"version_minor":0},"text/plain":["spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"57533279df7c41198473387072fd3d24","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/874M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments\n","from transformers import DataCollatorForTokenClassification\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n","model = AutoModelForTokenClassification.from_pretrained(\"microsoft/deberta-v3-large\", num_labels=len(all_labels))\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-09-19T17:12:51.579059Z","iopub.status.busy":"2024-09-19T17:12:51.576483Z","iopub.status.idle":"2024-09-19T17:12:51.586211Z","shell.execute_reply":"2024-09-19T17:12:51.584752Z","shell.execute_reply.started":"2024-09-19T17:12:51.579023Z"},"trusted":true},"outputs":[{"data":{"text/plain":["['tokens']"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["test_dataset.column_names"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-09-19T17:12:51.587871Z","iopub.status.busy":"2024-09-19T17:12:51.587503Z","iopub.status.idle":"2024-09-19T17:12:51.633589Z","shell.execute_reply":"2024-09-19T17:12:51.632744Z","shell.execute_reply.started":"2024-09-19T17:12:51.587830Z"},"trusted":true},"outputs":[],"source":["# Tokenize and align the labels\n","def tokenize_and_align_labels(examples, labels_available=True, max_length=256):\n","    # Tokenize the inputs with truncation, padding, and a max_length\n","    tokenized_inputs = tokenizer(\n","        examples['tokens'],\n","        truncation=True,\n","        is_split_into_words=True,\n","        padding='max_length',  # Ensures padding to max_length\n","        max_length=max_length  # Set max_length to prevent overly long sequences\n","    )\n","\n","    if labels_available:\n","        all_labels = examples['ner_tags']\n","        new_labels = []\n","\n","        # Iterate over each set of labels\n","        for i, labels in enumerate(all_labels):\n","            word_ids = tokenized_inputs.word_ids(batch_index=i)\n","            aligned_labels = align_labels_with_tokens(labels, word_ids)\n","\n","            # Print debug information to help troubleshoot length mismatches\n","            if len(aligned_labels) != len(word_ids):\n","                print(f\"Length mismatch at index {i}: Tokens length = {len(word_ids)}, Labels length = {len(aligned_labels)}\")\n","\n","            new_labels.append(aligned_labels)\n","\n","        # Map labels to their respective IDs (make sure to include all label types)\n","        label_to_id = {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-LOC': 3, 'I-LOC': 4, -100: -100}  # Extend as needed\n","        new_labels = [[label_to_id.get(label, -100) for label in label_list] for label_list in new_labels]\n","\n","        tokenized_inputs[\"labels\"] = new_labels\n","\n","    return tokenized_inputs"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-09-19T17:12:51.635039Z","iopub.status.busy":"2024-09-19T17:12:51.634690Z","iopub.status.idle":"2024-09-19T17:13:19.718591Z","shell.execute_reply":"2024-09-19T17:13:19.717676Z","shell.execute_reply.started":"2024-09-19T17:12:51.634997Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3dfdf4861dc443d7a9ce71987d3c64f6","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/6807 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c3766bf5c67442f08ad6606c4a4d4563","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/10 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Apply the tokenization and label alignment for the training set (with labels)\n","train_dataset = train_dataset.map(lambda x: tokenize_and_align_labels(x, labels_available=True), batched=True)\n","\n","# Apply the tokenization for the test set (without labels)\n","test_dataset = test_dataset.map(lambda x: tokenize_and_align_labels(x, labels_available=False), batched=True)\n"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-09-19T17:13:19.720208Z","iopub.status.busy":"2024-09-19T17:13:19.719878Z","iopub.status.idle":"2024-09-19T17:13:19.732358Z","shell.execute_reply":"2024-09-19T17:13:19.731412Z","shell.execute_reply.started":"2024-09-19T17:13:19.720174Z"},"trusted":true},"outputs":[],"source":["from seqeval.metrics import accuracy_score, f1_score, classification_report\n","\n","def compute_metrics(pred):\n","    labels = pred.label_ids\n","    preds = pred.predictions.argmax(-1)\n","\n","    # Remove ignored index (special tokens like -100)\n","    true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n","    true_preds = [[id2label[p] for (p, l) in zip(pred, label) if l != -100] for pred, label in zip(preds, labels)]\n","\n","    return {\n","        \"accuracy\": accuracy_score(true_labels, true_preds),\n","        \"f1\": f1_score(true_labels, true_preds),\n","        \"classification_report\": classification_report(true_labels, true_preds)\n","    }"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-09-19T17:13:19.733691Z","iopub.status.busy":"2024-09-19T17:13:19.733425Z","iopub.status.idle":"2024-09-19T17:13:19.823956Z","shell.execute_reply":"2024-09-19T17:13:19.823021Z","shell.execute_reply.started":"2024-09-19T17:13:19.733661Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]}],"source":["from transformers import Trainer, TrainingArguments\n","\n","\n","# Training arguments\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    evaluation_strategy=\"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    num_train_epochs=1,\n","    weight_decay=0.01,\n","    fp16=True,  # Enable mixed precision\n","    gradient_accumulation_steps=4\n",")"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-09-19T17:13:19.826039Z","iopub.status.busy":"2024-09-19T17:13:19.825350Z","iopub.status.idle":"2024-09-19T17:13:21.138510Z","shell.execute_reply":"2024-09-19T17:13:21.137721Z","shell.execute_reply.started":"2024-09-19T17:13:19.825979Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"]}],"source":["# Trainer setup\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=test_dataset\n",")"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-09-19T17:13:21.139945Z","iopub.status.busy":"2024-09-19T17:13:21.139640Z","iopub.status.idle":"2024-09-19T17:26:38.304916Z","shell.execute_reply":"2024-09-19T17:26:38.303883Z","shell.execute_reply.started":"2024-09-19T17:13:21.139911Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/html":["wandb version 0.18.1 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.17.7"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240919_171328-21p20wy9</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/wangdeismail-mpstme/huggingface/runs/21p20wy9' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/wangdeismail-mpstme/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/wangdeismail-mpstme/huggingface' target=\"_blank\">https://wandb.ai/wangdeismail-mpstme/huggingface</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/wangdeismail-mpstme/huggingface/runs/21p20wy9' target=\"_blank\">https://wandb.ai/wangdeismail-mpstme/huggingface/runs/21p20wy9</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='212' max='212' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [212/212 12:48, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>No log</td>\n","      <td>No log</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=212, training_loss=0.06464113829270848, metrics={'train_runtime': 795.6044, 'train_samples_per_second': 8.556, 'train_steps_per_second': 0.266, 'total_flos': 3150312945942528.0, 'train_loss': 0.06464113829270848, 'epoch': 0.9964747356051704})"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","torch.cuda.empty_cache()\n","trainer.train()"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-09-19T17:26:38.308803Z","iopub.status.busy":"2024-09-19T17:26:38.308488Z","iopub.status.idle":"2024-09-19T17:26:38.728661Z","shell.execute_reply":"2024-09-19T17:26:38.727510Z","shell.execute_reply.started":"2024-09-19T17:26:38.308770Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n","Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"]},{"name":"stdout","output_type":"stream","text":["[{'entity_group': 'LABEL_0', 'score': 0.99993885, 'word': \"John Doe's email is john.doe@example.com and his phone number is 123-456-7890.\", 'start': 0, 'end': 78}]\n"]}],"source":["from transformers import pipeline\n","\n","# Load NER pipeline with trained model\n","ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n","\n","# Example of using the pipeline\n","text = \"John Doe's email is john.doe@example.com and his phone number is 123-456-7890.\"\n","ner_results = ner_pipeline(text)\n","\n","print(ner_results)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":7500999,"sourceId":66653,"sourceType":"competition"}],"dockerImageVersionId":30761,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
